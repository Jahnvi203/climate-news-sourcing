{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Extractive Summarisation**"
      ],
      "metadata": {
        "id": "hwQNj332-cVd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6E3kwU1fX4"
      },
      "source": [
        "# Libraries & Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPd7L_B_Q8OZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install pytextrank\n",
        "import pytextrank\n",
        "!python -m spacy download en_core_web_md\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tag import pos_tag\n",
        "from heapq import nlargest\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe(\"textrank\")\n",
        "stops = stopwords.words('english')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5QtlHeNTF_w"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"all_articles_cleaned_no_unnecessary_words.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYwRQ7DzUvon"
      },
      "outputs": [],
      "source": [
        "df['body'] = df['body'].str.replace(\"\\n\", \"\")\n",
        "df['body'] = df['body'].str.replace(\"  \", \"\")\n",
        "df['body'] = df['body'].str.replace(\"\\'\", \"'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.iloc[:1000]"
      ],
      "metadata": {
        "id": "pAmNmq3umG3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYwkgmZxnQA7",
        "outputId": "2003d8a7-8f1e-486c-8f8e-5a876ca11819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                   float64\n",
              "body                  object\n",
              "headline              object\n",
              "article_name          object\n",
              "article_url           object\n",
              "date_published        object\n",
              "article_length         int64\n",
              "date_uploaded         object\n",
              "article_start_url     object\n",
              "source                object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['body'] = df['body'].values.astype(str)"
      ],
      "metadata": {
        "id": "aUZSbxE-nG97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['headline'] = df['headline'].values.astype(str)"
      ],
      "metadata": {
        "id": "lOMNGhXynPkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLrC8DcTTdyY"
      },
      "source": [
        "# Lexical Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcAk0bcLSPOL"
      },
      "outputs": [],
      "source": [
        "def noun_relations(nouns):\n",
        "    relation_list = defaultdict(list)\n",
        "    for i in range(len(nouns)):\n",
        "        relation = []\n",
        "        for syn in wordnet.synsets(nouns[i], pos = wordnet.NOUN):\n",
        "            for a in syn.lemmas():\n",
        "                relation.append(a.name())\n",
        "                if a.antonyms():\n",
        "                    relation.append(a.antonyms()[0].name())\n",
        "            for b in syn.hyponyms():\n",
        "                if b.hyponyms():\n",
        "                    relation.append(b.hyponyms()[0].name().split('.')[0])\n",
        "            for c in syn.hypernyms():\n",
        "                if c.hypernyms():\n",
        "                    relation.append(c.hypernyms()[0].name().split('.')[0])\n",
        "        relation_list[nouns[i]].append(relation)\n",
        "    return relation_list\n",
        "\n",
        "def generate_lexical_chain(nouns, relation_list):\n",
        "    lexical = []\n",
        "    threshold = 0.5\n",
        "    for noun in nouns:\n",
        "        flag = 0\n",
        "        for j in range(len(lexical)):\n",
        "            if flag == 0:\n",
        "                for key in list(lexical[j]):\n",
        "                    if key == noun and flag == 0:\n",
        "                        lexical[j][noun] +=1\n",
        "                        flag = 1\n",
        "                    elif key in relation_list[noun][0] and flag == 0:\n",
        "                        syns1 = wordnet.synsets(key, pos = wordnet.NOUN)\n",
        "                        syns2 = wordnet.synsets(noun, pos = wordnet.NOUN)\n",
        "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
        "                            lexical[j][noun] = 1\n",
        "                            flag = 1\n",
        "                    elif noun in relation_list[key][0] and flag == 0:\n",
        "                        syns1 = wordnet.synsets(key, pos = wordnet.NOUN)\n",
        "                        syns2 = wordnet.synsets(noun, pos = wordnet.NOUN)\n",
        "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
        "                            lexical[j][noun] = 1\n",
        "                            flag = 1\n",
        "        if flag == 0: \n",
        "            new_dict = {}\n",
        "            new_dict[noun] = 1\n",
        "            lexical.append(new_dict)\n",
        "            flag = 1\n",
        "    return lexical\n",
        "\n",
        "def prune(lexical):\n",
        "    final_chain = []\n",
        "    while lexical:\n",
        "        result = lexical.pop()\n",
        "        if len(result.keys()) == 1:\n",
        "            for value in result.values():\n",
        "                if value != 1: \n",
        "                    final_chain.append(result)\n",
        "        else:\n",
        "            final_chain.append(result)\n",
        "    return final_chain\n",
        "\n",
        "threshold_min = 0.1\n",
        "threshold_max = 0.9\n",
        "\n",
        "def return_frequencies(words, lexical_chain):\n",
        "    frequencies = defaultdict(int)\n",
        "    for word in words:\n",
        "        for w in word:\n",
        "            if w not in stops:\n",
        "                flag = 0\n",
        "                for i in lexical_chain:\n",
        "                    if w in list(i.keys()):\n",
        "                        frequencies[w] = sum(list(i.values()))\n",
        "                        flag = 1\n",
        "                        break\n",
        "                if flag == 0: \n",
        "                    frequencies[w] += 1\n",
        "    m = float(max(frequencies.values()))\n",
        "    for w in list(frequencies.keys()):\n",
        "        frequencies[w] = frequencies[w]/m\n",
        "        if frequencies[w] >= threshold_max or frequencies[w] <= threshold_min:\n",
        "            del frequencies[w]\n",
        "    return frequencies\n",
        "\n",
        "def summarize(sentence, lexical_chain, n):\n",
        "    assert n <= len(sentence)\n",
        "    word_sentence = [word_tokenize(s.lower()) for s in sentence]\n",
        "    frequencies = return_frequencies(word_sentence, lexical_chain)\n",
        "    ranking = defaultdict(int)\n",
        "    for i, sent in enumerate(word_sentence):\n",
        "        for word in sent:\n",
        "            if word in frequencies:\n",
        "                ranking[i] += frequencies[word]\n",
        "                idx = rank(ranking, n)\n",
        "    final_index = sorted(idx)\n",
        "    return [sentence[j] for j in final_index]\n",
        "\n",
        "def rank(ranking, n):\n",
        "    return nlargest(n, ranking, key=ranking.get)\n",
        "\n",
        "position = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def generate_lexical_chain_summary(input_content):\n",
        "    sentence = sent_tokenize(input_content)\n",
        "    tokens = [tokenizer.tokenize(w) for w in sentence]\n",
        "    tagged = [pos_tag(tok) for tok in tokens]\n",
        "    nouns = [word.lower() for i in range(len(tagged)) for word, pos in tagged[i] if pos in position]\n",
        "    relation = noun_relations(nouns)\n",
        "    lexical = generate_lexical_chain(nouns, relation)\n",
        "    final_chain = prune(lexical)\n",
        "    if len(sentence) >= 3:\n",
        "        n = 3\n",
        "    else: \n",
        "        n = 1\n",
        "    s = summarize(sentence, final_chain, n)\n",
        "    return [final_chain, s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDh9nsmga_uQ"
      },
      "outputs": [],
      "source": [
        "def lc_summ(df):\n",
        "  try:\n",
        "    if type(df['body']) == str:\n",
        "      return \" \".join(generate_lexical_chain_summary(df['body'])[1])\n",
        "    else:\n",
        "      return \"\"\n",
        "  except:\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EQ4juc6aY71"
      },
      "outputs": [],
      "source": [
        "df['lc_summ'] = df.apply(lc_summ, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA6S1UPZUp8k"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrcEr506Y3Nl"
      },
      "outputs": [],
      "source": [
        "def basic_preprocess(self_text):\n",
        "     # 1. Remove html tags\n",
        "    words = BeautifulSoup(self_text).get_text()\n",
        "    # 2. Convert words to lower case and split each word up\n",
        "    words = self_text.lower()\n",
        "    # 3. Remove non-letters aka punctuation\n",
        "    words = re.sub(\"[^a-zA-Z]\", \" \", words).split()    \n",
        "    # 4 Remove stopwords\n",
        "    words = [word for word in words if word not in stops]\n",
        "    # 5 LEMMATIZE!\n",
        "    words = [lemmatizer.lemmatize(w) for w in words]\n",
        "    # 7. Join words back into one string, with a space in between each word\n",
        "    return(\" \".join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1qfV22RUN8O"
      },
      "outputs": [],
      "source": [
        "def top_sentence(input_doc, limit):\n",
        "    keyword = []\n",
        "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
        "    doc = nlp(input_doc)\n",
        "    processed_doc = nlp(basic_preprocess(input_doc))\n",
        "    for token in processed_doc:\n",
        "        if token.pos_ in pos_tag:\n",
        "            keyword.append(token.text)\n",
        "    freq_word = Counter(keyword)\n",
        "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
        "    for w in freq_word:\n",
        "        freq_word[w] = freq_word[w] / max_freq\n",
        "    sent_strength = {}\n",
        "    for sent in doc.sents:\n",
        "        for word in sent:\n",
        "            if word.text in freq_word.keys():\n",
        "                if sent in sent_strength.keys():\n",
        "                    sent_strength[sent] += freq_word[word.text]\n",
        "                else:\n",
        "                    sent_strength[sent] = freq_word[word.text]\n",
        "    summary = []\n",
        "    sorted_x = sorted(sent_strength.items(), key = lambda kv: kv[1], reverse = True)\n",
        "    counter = 0\n",
        "    for i in range(len(sorted_x)):\n",
        "        summary.append(str(sorted_x[i][0]).capitalize())\n",
        "        counter += 1\n",
        "        if(counter >= limit):\n",
        "            break\n",
        "    return ' '.join(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXDu7Np4U_Tb"
      },
      "outputs": [],
      "source": [
        "def tf_idf_summ(df):\n",
        "  try:\n",
        "    if type(df['body']) == str:\n",
        "      return top_sentence(df['body'], 3)\n",
        "    else:\n",
        "      return \"\"\n",
        "  except:\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl4EpNQpU6wp"
      },
      "outputs": [],
      "source": [
        "df['tf_idf_summ'] = df.apply(tf_idf_summ, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnCoRHaoCog2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI7z1CJeVZuf"
      },
      "source": [
        "# TextRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb2RDQ6UVbNx"
      },
      "outputs": [],
      "source": [
        "def tr_summ(df):\n",
        "  try:\n",
        "    if type(df['body']) == str:\n",
        "      summary = []\n",
        "      for sent in nlp(df['body'])._.textrank.summary(limit_phrases = 15, limit_sentences = 3):\n",
        "        summary.append(str(sent))\n",
        "      return \" \".join(summary)\n",
        "    else:\n",
        "      return \"\"\n",
        "  except:\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT77utIYbS8q"
      },
      "outputs": [],
      "source": [
        "df['tr_summ'] = df.apply(tr_summ, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstractive Summarisation**"
      ],
      "metadata": {
        "id": "zeggGqmY-n80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries & Packages"
      ],
      "metadata": {
        "id": "lgNUGZBg-vBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "import sentencepiece"
      ],
      "metadata": {
        "id": "qbMcmyz_-shO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bodies = df['body'].values.tolist()"
      ],
      "metadata": {
        "id": "ZEz42f62gh-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pegasus"
      ],
      "metadata": {
        "id": "-UQw4U40_Nmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/pegasus-xsum'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
      ],
      "metadata": {
        "id": "3RwTNH4jgQBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer.prepare_seq2seq_batch(bodies, truncation = True, padding = 'longest', return_tensors ='pt')\n",
        "translated = model.generate(**batch)\n",
        "output = tokenizer.batch_decode(translated, skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "kcG0ka1sgczb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['ab_summ'] = output"
      ],
      "metadata": {
        "id": "kgRsdC81o5hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVeACw9Q1Hu0"
      },
      "source": [
        "# **Summaries exported to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foO9j3eu1AQP"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"articles with summaries.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "YY3VMaTEpCPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "jVZJJYUVpImK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "E4bzCKxHpPTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lc_predictions = df['lc_summ'].values.tolist()\n",
        "tf_idf_predictions = df['tf_idf_summ'].values.tolist()\n",
        "tr_predictions = df['tr_summ'].values.tolist()\n",
        "ab_predictions = df['ab_summ'].values.tolist()\n",
        "references = df['headline'].values.tolist()"
      ],
      "metadata": {
        "id": "-jPsaA82pUS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric = Rouge()\n",
        "eval_results_lc = eval_metric.get_scores(lc_predictions, references, avg = True)\n",
        "eval_results_tf_idf = eval_metric.get_scores(tf_idf_predictions, references, avg = True)\n",
        "eval_results_tr = eval_metric.get_scores(tr_predictions, references, avg = True)\n",
        "eval_results_ab = eval_metric.get_scores(ab_predictions, references, avg = True)"
      ],
      "metadata": {
        "id": "hIktmwgopuUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(eval_results_lc)"
      ],
      "metadata": {
        "id": "qySe6uDbpxVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(eval_results_tf_idf)"
      ],
      "metadata": {
        "id": "t4jJEnNbpzne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(eval_results_tr)"
      ],
      "metadata": {
        "id": "Rr-E5HulqLdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(eval_results_ab)"
      ],
      "metadata": {
        "id": "zlS9NI02qN9J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vO6E3kwU1fX4",
        "xLrC8DcTTdyY",
        "EA6S1UPZUp8k",
        "OI7z1CJeVZuf",
        "lgNUGZBg-vBk",
        "-UQw4U40_Nmu",
        "CVeACw9Q1Hu0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}